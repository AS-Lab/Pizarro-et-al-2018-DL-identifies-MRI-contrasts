"""
=======================================
Load and train Dense Neural Network
=======================================

We developed two sequential neural networks to identify the brain MRI contrast.  The first network was a convolutional neural network that inferred the modality on a sagittal slice.  The second network combined the result generated by the first nerwork to infer the modality on the entire volume.  

This script was used to train the second, dense neural network.  The model architecture was loaded from .json string file saved using modality.save_NNarch_toJson.py.  The dataset was previously divided into three sets: training, validation, and testing.  The training set was used to estimate the model parameters.  The validation  set was used to estimate performance after each epoch completed.  The testing set was used in a different script to test the model after training completed. 

The training dataset was too large to load at once so a file_generator was used to generate data as needed by Keras' fit_generator.  The training parameters such as number of epochs, number of steps per epoch, and number of samples per steps were determined emperically and can easily be changed by the user.  The file_generator populates the numpy arrays needed to train the model from randomly selected MRI volumes within the set.  The input to this network is generated by the first network, CNN.

During the training the weights for the model parameters are saved if performance (accuracy and loss) is improved.  Upon completion this script saves the final weights of the parameters after the number of epochs specified.

"""
print(__doc__)

import numpy as np
import nibabel as nib
import json

np.seterr(all='raise')

from keras.models import Model, model_from_json
from keras.callbacks import ModelCheckpoint, History
from keras.utils import np_utils


def get_files(fn):
    # fn is filename
    with open(fn) as f:
        # files are loaded as tuples with modality and filenames
        files = [tuple(i.split(' ')) for i in f]
    return files


def train_DNN(train_files,valid_files,NN,nb_mods,nb_step,input_size):
    # this function loads the neural network architecture, trains
    # the neural network, saves the weights that improve performance,

    # the model was defined and saved in modality.save_NNarch_toJson.py
    model=get_model(NN,nb_mods,verbose=True)

    # function to approximate output CNN based on model and estimated weights
    CNN_func = get_CNN_func(nb_mods=nb_mods)
    # syntax to save the weights that improve performance
    checkpath='weights/weights.{epoch:04d}_loss_{loss:0.2f}.h5'
    checkpointer=ModelCheckpoint(checkpath, monitor='loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto')
    # track performance (accuracy and loss) on train and validation datasets
    performance = History()
    # train the neural network by estimating model parameters that minimize loss
    # determined emperically: nb_step, steps_per_epoch, and epochs
    testing_batch = dense_generator(valid_files,CNN_func,nb_step=nb_step,verbose=False,nb_mods=nb_mods).next()
    model.fit_generator(dense_generator(train_files,CNN_func,nb_step=nb_step,verbose=False,nb_mods=nb_mods), steps_per_epoch=20, epochs=1000, verbose=1,
        validation_data=testing_batch, validation_steps=1, callbacks=[performance,checkpointer])
    # save the final weights after the training session completes
    model.save_weights('weights/weights.FINAL.h5',overwrite=True)
    # save the performance (accuracy and loss) history
    save_history(performance)


def get_model(NN='DNN',nb_mods=5,verbose=False):
    # load the architecture defined and saved in modality.save_NNarch_toJson.py
    fn = "./model/{0}_{1}mod.json".format(NN,nb_mods)
    # the model architecture is loaded from a .json file
    with open(fn) as json_data:
        d = json.load(json_data)
    model = model_from_json(d)
    # compile the model architecture to ensure the dimensions match and are correct
    model.compile(loss='categorical_crossentropy',optimizer='nadam',metrics=['accuracy'])
    # print to screen the model architecture, if desired by user
    if verbose:
        print(model.summary())
    return model


def get_CNN_func(nb_mods=5):
    # CNN_func is a function that estimates the CNN model with the loaded weights
    # given a sagittal slice as input CNN_func generates the same output as CNN

    # get the CNN model architecture
    NN='CNN'
    model=get_model(NN,nb_mods)
    # load the weights saved after training the CNN model
    weights_fn = './weights/weights.{0}.{1}mod.h5'.format(NN,nb_mods)
    model.load_weights(weights_fn)
    # select the final layer by name, edit to generate the output of another layer
    layer_name = 'soft_026'
    CNN_func = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)

    return CNN_func


def save_history(base,h):
    # use json format to save accuracy, loss over epoch
    json_string=json.dumps(h.history)
    fn=base+'history_parms.json'
    with open(fn, 'w') as outfile:
        json.dump(json_string, outfile)
    

def dense_generator(files, CNN_func, nb_step=30, verbose=False, nb_mods=5):
    # number of slices to use from each MRI volume to generate CNN output
    nb_slices = 30
    # number of subjects needed to populate the numpy arrays X,Y
    nb_subj = nb_step/nb_slices
    # X is generated by the reshaping the CNN output of nb_slices from each volume
    X = np.zeros((nb_subj, nb_slices*nb_mods))
    # Y contains the true modalities by volume of the same MRI volumes in X
    Y = np.zeros((nb_subj, nb_mods), dtype=int)
    # infinite loop needed to continue generating data whle training
    while True:
        # n is the subject number or MRI volume being looped over
        n = 0
        # populate X and Y until the required nb_step is reached
        while n < nb_subj:
            # catch problematic MRI volumes
            try:
                # randomly select one volume from the list of files
                idx = np.random.randint(0, len(files))
                # modality categorized as a number and read from file
                mod = int(files[idx][0])
                # filename read from files
                f = files[idx][1].strip()
                # ensure the read mod is within the nb_mods maximum value
                if mod < nb_mods:
                    # if desired print to screen the modality and filename
                    if verbose:
                        print("{} : {}".format(mod, f))
                    # get the CNN output for nb_slices sagittal slices
                    try:
                        data = gen_final_layer(f,CNN_func,nb_slices,nb_mods).flatten()
                    except Exception as e:
                        print("Warning: {} {}".format(e, f))
                        continue
                    # catch for NaN values in the MRI volume
                    if not np.all(np.isfinite(data)):
                        print("Loaded NaN values with {}".format(f))
                        continue
                    # populate X and Y with data and modality as category vector
                    X[n,:] = data
                    Y[n,:] = np_utils.to_categorical([mod], nb_mods)
                    # increase n by 1 subject
                    n += 1
            # catch error and print
            except Exception, e:
                print(str(e))
                pass
        # yield X,Y to fit_generator()
        yield X, Y


def gen_final_layer(img_fn,CNN_func,nb_slices,nb_mods=5):

    # load file as a numpy array using nibabel
    img = nib.load(img_fn).get_data()
    # reorder and invert axes
    img = np.swapaxes(img,0,2)
    img = img[::-1, ::-1, :]

    # in case the volume does not have enough sagittal slices
    if nb_slices>img.shape[0]:
        nb_slices=img.shape[0]

    # input to CNN in order to pass into CNN_func and generate CNN_out
    X_nb_slices = get_img_data(img,nb_slices)
    CNN_out=np.zeros((1,nb_slices,nb_mods))

    # populate CNN_out with prediction from CNN_func on each slice
    for n in range(nb_slices):
        layer_out = CNN_func.predict(X_nb_slices[n])
        CNN_out[0,n,:]= layer_out[0,:]
    return CNN_out


def get_img_data(img,nb_slices=30):
    # Get image data and normalize
    img = np.array(img).astype('float32')
    img = grab_sagittal(img,nb_slices)
    img = reshape_dimension(img)
    return img


def grab_sagittal(img,nb_slices):
    # extract sagittal nb_slices from img and preprocess
    # total number of sagittal slices in the MRI volume
    x_total = img.shape[0]
    # middle of the volume in the sagittal direction
    x_mid = np.around(x_total / 2).astype(int)
    # sagittal_volume is a list to populate in for loop and returned as numpy array
    sagittal_volume=[]
    # window is nb_slices divided by 2
    window = np.round(nb_slices / 2).astype(int)
    for x_idx in range(x_mid-window,x_mid+window):
        # sagittal slice extraced from img
        slice_sagittal=img[x_idx,:,:]
        # resample the sagittal slice to 32x32
        slice_resampled=resample_slice(slice_sagittal)
        # intensity normalize each slice
        slice_normalized=normalize(slice_resampled)
        # append to volume slice by slice
        sagittal_volume.append(slice_normalized)
    # return as a numpy array
    return np.asarray(sagittal_volume)


def resample_slice(dSlice):
    # size of the slice
    (Ny,Nz)=dSlice.shape
    # resample each slice to make it 32x32
    s=np.linspace(0,31,32)/32
    # sample in the y-direction
    sy=np.around(s*Ny).astype(int)
    tmp=dSlice[sy,:]
    # sample in the z-direction
    sz=np.around(s*Nz).astype(int)
    return tmp[:,sz]


def normalize(slice_array):
    # slice_array is intensity normalized by mean and variance
    # mean of the slice
    m=np.mean(slice_array)
    # standard deviation of the slice_array
    st=np.std(slice_array)
    slice_normalized = (slice_array - m) / st
    return slice_normalized


def reshape_dimension(img):
    # reorder the dimensions for the neural network architecture
    (x,y,z)=img.shape
    return np.reshape(img,(x,1,y,z))



# the entire dataset was split into three groups:
# train: list of files used for training the algorithm
# valid: list of files used for validation after each training epoch
# test : list of files used for testing the algorithm after training

# this code uses the files located in training and validation groups

train_fn = './cross_valid_fns/train.filenames.txt'
valid_fn = './cross_valid_fns/valid.filenames.txt'

train_files=get_files(train_fn)
valid_files=get_files(valid_fn)

# select the dense neural network (DNN)
NN='DNN'
# number of modalities: [5,8] can be altered
nb_mods=5
# number of sagittal slices used by the neural network
# while taking a training step and validation step
nb_step=100
# the dimension of one sagittal slice
input_size=(1,32,32)
# the size for the training dataset
train_size=len(train_files)
# print to the screen information regarding the training session
print('\n==Training {0}, number of modalities:{1}, dataset size:{2}==\n'.format(NN,nb_mods,train_size))
# execute training command
train_DNN(train_files,valid_files,NN,nb_mods,nb_step,input_size)
